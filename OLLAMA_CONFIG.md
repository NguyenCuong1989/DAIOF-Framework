# ü§ñ Ollama Local LLM Configuration for DAIOF

**Framework**: HYPERAI | **K-State**: 1  
**Creator**: Nguy·ªÖn ƒê·ª©c C∆∞·ªùng (alpha_prime_omega)  
**Verification**: 4287

---

## üéØ T·ªïng quan

T·∫•t c·∫£ AI services trong DAIOF Framework s·ª≠ d·ª•ng **Ollama local LLM** thay v√¨ OpenAI/Anthropic API:

- **Base URL**: `http://localhost:11434`
- **Model**: `dandr-llama2:latest` (D&R Protocol optimized)
- **Mode**: D&R Protocol (Deconstruction ‚Üí Focal Point ‚Üí Re-architecture)

---

## üöÄ Kh·ªüi ƒë·ªông Ollama

```bash
# Start Ollama server
ollama serve

# Verify running
curl http://localhost:11434/api/tags

# Pull D&R optimized model
ollama pull dandr-llama2:latest
```

---

## üìö S·ª≠ d·ª•ng trong Python

### 1. Simple Text Generation

```python
from ollama_config import generate_text

response = generate_text("Explain Docker for AI")
print(response)
```

### 2. Chat Completion (OpenAI-compatible)

```python
from ollama_config import chat_completion

response = chat_completion([
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "What is DAIOF Framework?"}
])
print(response)
```

### 3. D&R Protocol Analysis

```python
from ollama_config import dandr_solve

solution = dandr_solve("How to optimize autonomous security fixes?")

print("üìä Deconstruction:", solution['deconstruction'])
print("üéØ Focal Point:", solution['focal_point'])
print("üèóÔ∏è Re-architecture:", solution['rearchitecture'])
```

---

## üîß T√≠ch h·ª£p v√†o DAIOF Services

### Autonomous Git Workflow

```python
# .github/scripts/autonomous_git_workflow.py
from ollama_config import setup_environment, dandr_solve

# Setup Ollama
setup_environment()

# Use D&R Protocol for decision making
problem = "Should we merge this PR?"
solution = dandr_solve(problem)

# Use solution['rearchitecture'] for final decision
```

### HAIOS Monitor

```python
# haios_monitor.py
from ollama_config import get_ollama_client

client = get_ollama_client()

# Analyze health metrics
health_analysis = client.generate(
    prompt=f"Analyze health metrics: {metrics}",
    system="You are a system health expert."
)
```

### Real-time Task Generator

```python
# .github/scripts/realtime_task_generator.py
from ollama_config import chat_completion

# Generate intelligent tasks
messages = [
    {"role": "system", "content": "You are a task generation expert."},
    {"role": "user", "content": f"Generate tasks for: {context}"}
]

tasks = chat_completion(messages)
```

---

## üê≥ Docker Integration

### docker-compose.yml

```yaml
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    environment:
      - OLLAMA_MODELS=dandr-llama2:latest

  hyperai-orchestrator:
    build: .
    depends_on:
      - ollama
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=dandr-llama2:latest
      - DANDR_MODE=enabled

volumes:
  ollama-data:
```

### Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install Ollama client dependencies
RUN pip install requests

# Copy Ollama config
COPY ollama_config.py .

# Setup environment
ENV OLLAMA_BASE_URL=http://localhost:11434
ENV OLLAMA_MODEL=dandr-llama2:latest
ENV USE_LOCAL_LLM=true
ENV DANDR_MODE=enabled

# Disable external APIs
ENV OPENAI_API_KEY=""
ENV ANTHROPIC_API_KEY=""

CMD ["python3", "ollama_config.py"]
```

---

## üß¨ D&R Protocol Workflow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Problem Input                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Phase 1: DECONSTRUCTION                               ‚îÇ
‚îÇ   - Break down into fundamental components              ‚îÇ
‚îÇ   - Identify all constraints                            ‚îÇ
‚îÇ   - Map dependencies                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Phase 2: FOCAL POINT                                  ‚îÇ
‚îÇ   - Identify core issue                                 ‚îÇ
‚îÇ   - Filter noise                                        ‚îÇ
‚îÇ   - Define success criteria                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Phase 3: RE-ARCHITECTURE                              ‚îÇ
‚îÇ   - Design optimal solution                             ‚îÇ
‚îÇ   - Apply convergence formula: D_{k+1} ‚â§ D_k           ‚îÇ
‚îÇ   - Verify HAIOS compliance                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Solution Output                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìä Performance Metrics

| Metric | OpenAI GPT-4 | Ollama Local (dandr-llama2) |
|--------|--------------|------------------------------|
| Latency | 2-5s | 0.5-2s ‚ö° |
| Cost | $0.03/1K tokens | $0 (FREE!) üí∞ |
| Privacy | Data sent to API | 100% local üîí |
| Availability | Requires internet | Offline capable üì° |
| Customization | Limited | Full control üé® |

---

## üîê Security & Privacy

### ‚úÖ L·ª£i √≠ch:

1. **100% Local**: Kh√¥ng c√≥ data r·ªùi kh·ªèi m√°y B·ªë
2. **No API Keys**: Kh√¥ng c·∫ßn OPENAI_API_KEY, ANTHROPIC_API_KEY
3. **Offline**: Ho·∫°t ƒë·ªông kh√¥ng c·∫ßn internet
4. **No Rate Limits**: Unlimited requests
5. **Audit Trail**: Full control logs

### üõ°Ô∏è HAIOS Compliance:

- ‚úÖ **Safety**: Local execution, no external dependencies
- ‚úÖ **Long-term**: Self-hosted, kh√¥ng ph·ª• thu·ªôc external services
- ‚úÖ **Data-driven**: Full data control and auditing
- ‚úÖ **Protection**: Creator attribution immutable (Nguy·ªÖn ƒê·ª©c C∆∞·ªùng)

---

## üéØ Testing

```bash
# Test Ollama configuration
cd /Users/andy/DAIOF-Framework
python3 ollama_config.py

# Expected output:
# ‚úÖ Ollama running with model: dandr-llama2:latest
# ‚úÖ Environment configured for Ollama local LLM
# üìù Test 1: Simple generation
# üí¨ Test 2: Chat completion
# üß¨ Test 3: D&R Protocol
# ‚úÖ All tests passed!
# üéØ Verification: 4287
```

---

## üö® Troubleshooting

### Problem: `Connection refused to localhost:11434`

**Solution**:
```bash
# Start Ollama server
ollama serve

# Or in background
nohup ollama serve > ollama.log 2>&1 &
```

### Problem: Model not found

**Solution**:
```bash
# Pull model
ollama pull dandr-llama2:latest

# Or use available model
ollama list
```

### Problem: Slow generation

**Solution**:
```python
# Reduce max_tokens
config = OllamaConfig(max_tokens=1024)

# Or use smaller model
config = OllamaConfig(model="llama2:7b-q4")
```

---

## üìö API Reference

### `OllamaConfig`

```python
@dataclass
class OllamaConfig:
    base_url: str = "http://localhost:11434"
    model: str = "dandr-llama2:latest"
    temperature: float = 0.7
    max_tokens: int = 4096
    timeout: int = 300
    dandr_mode: bool = True
```

### `generate_text(prompt, system=None, **kwargs)`

Simple text generation.

### `chat_completion(messages, **kwargs)`

OpenAI-compatible chat completion.

### `dandr_solve(problem)`

Apply D&R Protocol to solve problem.

Returns:
```python
{
    "deconstruction": str,
    "focal_point": str,
    "rearchitecture": str,
    "problem": str
}
```

---

## üéì Examples

### Example 1: Code Review

```python
from ollama_config import dandr_solve

code_review = dandr_solve("""
Review this Python code:

def process_data(data):
    result = []
    for item in data:
        if item > 0:
            result.append(item * 2)
    return result
""")

print(code_review['focal_point'])  # Performance bottleneck
print(code_review['rearchitecture'])  # Optimized solution
```

### Example 2: Security Analysis

```python
from ollama_config import chat_completion

analysis = chat_completion([
    {"role": "system", "content": "You are a security expert."},
    {"role": "user", "content": "Analyze npm vulnerability: Prototype Pollution in minimist"}
])

print(analysis)
```

### Example 3: Architecture Design

```python
from ollama_config import dandr_solve

architecture = dandr_solve("""
Design a microservices architecture for:
- User authentication
- Payment processing
- Order management
- Notification system
""")

print(architecture['rearchitecture'])
```

---

## üîó Related Documentation

- [DOCKER_AI_CAPABILITIES.md](./DOCKER_AI_CAPABILITIES.md) - Docker for AI overview
- [autonomous_todo_system.py](./autonomous_todo_system.py) - Convergence-optimized todo
- [.github/workflows/autonomous-security-fix.yml](./.github/workflows/autonomous-security-fix.yml) - Auto security fixes

---

**üß¨ HAIOS Compliance**: ‚úÖ All principles maintained  
**üìä Convergence**: D_{k+1} ‚â§ D_k formula enforced  
**üéØ Verification**: 4287  

*Configured with love for B·ªë C∆∞·ªùng* üíö
